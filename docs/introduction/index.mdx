---
id: introduction
title: Introduction
description: Intro about Lumina-T2X
---

We introduce the Lumina-T2X family, a series of text-conditioned Diffusion Transformers (DiT) designed to convert noise into images, videos, multi-view images of 3D objects and generate speech based on textual instructions.

At the core of Lumina-T2X lies the Flow-based Large Diffusion Transformer (Flag-DiT), which supports scaling up to 7 billion parameters and extending sequence lengths up to 128,000. Inspired by Sora, Lumina-T2X integrates images, videos, multi-views of 3D objects, and speech spectrograms within a spatial-temporal latent token space. 

Lumina-T2X allows for the generation of outputs in any resolution, aspect ratio, and duration, facilitated by learnable `newline` and `newframe` tokens. Furthermore, training Lumina-T2X is computationally efficient. 

The largest model, with 5 billion parameters, requires only 20% of the training time needed for Pixart-alpha, which has 600 million parameters.

<img src="/img/framework/framework2.jpg"> </img>


## ðŸ›  Features

- Flow-based Large Diffusion Transformer (Flag-DiT): Lumina-T2X is trained **with flow matching pipeline**. For supporting training stability and model scalability, we support a bunch of techniques, such as RoPE, RMSNorm, and KQ-norm, **demonstrating faster training convergence, stable training dynamics, and a simplified pipeline**.

- Any Modalities, Res., and Duration within One Framework: 
  1. Lumina-T2X tokenizes images, videos, multi-views of 3D objects, and spectrograms into one-dimensional sequences. 
  2. Lumina-T2X can naturally **encode any modalityâ€”regardless of resolution, aspect ratios, and temporal durations into a unified 1-D token sequence** akin to LLMs, by utilizing Flag-DiT with text conditioning to iteratively transform noise into outputs across any modality, resolution, and duration during inference time. 
  3. Due to any modalityâ€”regardless of resolution, aspect ratios, and temporal durations encoding, it even **enables resolution extrapolation**, which allows the generation of resolutions out-of-domain that **were unseen during training**.

- Low Training Resources: increasing token length in transformers extends iteration times but **reduces overall training duration by decreasing the number of iterations needed**. Moreover, our Lumina-T2X model can generate high-resolution images and coherent videos **with minimal computational demands**. Remarkably, the default Lumina-T2I configuration, equipped with a 5 billion Flag-DiT and a 7 billion LLaMA as text encoder, **requires only $20\%$ of the computational resources needed by Pixelart-$\alpha$**.
