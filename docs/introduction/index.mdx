---
id: introduction
title: Introduction
description: Intro about Lumina-T2X
---

We introduce the Lumina-T2X family, a series of text-conditioned Diffusion Transformers (DiT) designed to convert noise into images, videos, multi-view images of 3D objects and generate speech based on textual instructions.

At the core of Lumina-T2X lies the Flow-based Large Diffusion Transformer (Flag-DiT), which supports scaling up to 7 billion parameters and extending sequence lengths up to 128,000. Inspired by Sora, Lumina-T2X integrates images, videos, multi-views of 3D objects, and speech spectrograms within a spatial-temporal latent token space. 

Lumina-T2X allows for the generation of outputs in any resolution, aspect ratio, and duration, facilitated by learnable `newline and `newframe tokens. Furthermore, training Lumina-T2X is computationally efficient. 

The largest model, with 5 billion parameters, requires only 20% of the training time needed for Pixart-alpha, which has 600 million parameters.


<img src="/img/framework/framework2.jpg"> </img>


## ðŸ›  Features

- Flow-based Large Diffusion Transformer (Flag-DiT): Diffusion Transformer (DiT) has been validated as an efficient denoiser backbone for generating high-fidelity images (DiT, SiT, and Pixelart-Alpha) and videos (Sora). Building on DiT, LLaMA, and flow matching, we propose a Flag-DiT as a scalable, efficient, and stable framework for generating high-resolution images and long video clips. We scale up the size of DiT from 600 million to 7 billion parameters, and increase the sequence length from 1K to 512K tokens, equipped with flow-matching to further improve the convergence speed. We believe Flag-DiT can serve as a universal foundational backbone for generative modeling across various modalities.

- Different Modalities, One Framework: Lumina-T2X introduces a unified approach by tokenizing diverse data types â€” ranging from images and videos to audio spectrograms â€” into consistent patch sequences. These multi-dimensional tokens are then transformed into one-dimensional sequences processed by Flag-DiT, enabling the framework to generate content across any modality from simple textual inputs. While we currently train individual generators for each modality due to data imbalance, our vision extends toward creating a universal generator capable of text-to-any generation in the future.

- Scaling Parameter and Token Length: The Flag-DiT framework enables us to scale parameters from 600M to 7B and extend the token length from 1K to 512K, maintaining stability throughout the training process. This improvement significantly boosts sample quality and unlocks new possibilities for generating extremely long and high-resolution images/videos. Furthermore, Lumina-T2X is provided with various model sizes to cater to diverse computational needs. 
- Generating Higher-Res Images Unseen during Training: Lumina-T2X exhibits exciting generalization capabilities that can produce higher resolution contents than its training data due to the introduction of RoPe and â€˜newline token, e.g. training on 1024*1024 images and generating 2048*2048 images. This unique feature is further augmented by advanced inference techniques, such as NTK-aware scaled rope, diffusion time shifting, and proportional attention, allowing for photorealistic, high-resolution generation without additional training.

- Low-computing Resources: Our empirical observations reveal that using larger models, high-resolution images, and extended video clips significantly boosts the convergence speed of diffusion models with improved visual quality and better image-text alignment. Although longer token length increases the duration per iteration due to the quadratic complexity of transformers, it significantly shortens the overall training time by decreasing the number of iterations needed. Additionally, by utilizing carefully curated text-image and text-video pairs with high aesthetic frames and detailed captions, our Lumina-T2X is capable of producing high-resolution images and coherent videos with minimal computing requirements.

- Support Model, Sequence Parallel and FSDP : The velocity prediction backbone of Lumina-T2X are highly deeply rooted from the architecture of Large Language Models (LLMs). Thus, it can  easily shard model weights, sequence length, optimizer states, weight storage and gradient communications across GPUs for scaling model size and sequence length. This practice can significantly increase the upper bound of model parameters and sequence length supported by Lumina-T2X framework. 

